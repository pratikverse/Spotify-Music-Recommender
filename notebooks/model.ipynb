{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Enhanced data preprocessing\n",
    "def load_and_preprocess_data():\n",
    "    ds = load_dataset(\"maharshipandya/spotify-tracks-dataset\")\n",
    "    df = pd.DataFrame(ds[\"train\"])\n",
    "    \n",
    "    print(\"‚úÖ Dataset loaded. First few rows:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "\n",
    "    # More robust encoding\n",
    "    le = LabelEncoder()\n",
    "    df['track_genre_encoded'] = le.fit_transform(df['track_genre'])\n",
    "\n",
    "    numeric_features = [\n",
    "        'danceability', 'energy', 'loudness', 'speechiness', \n",
    "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "        'track_genre_encoded' \n",
    "    ]\n",
    "    \n",
    "    # More thorough data cleaning\n",
    "    df = df.dropna(subset=numeric_features).reset_index(drop=True)\n",
    "    \n",
    "    # Remove outliers using IQR\n",
    "    for feature in numeric_features[:-1]:  # Skip the encoded genre\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        df = df[~((df[feature] < (Q1 - 1.5 * IQR)) | (df[feature] > (Q3 + 1.5 * IQR)))]\n",
    "    \n",
    "    X = df[numeric_features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return df, X_scaled, numeric_features\n",
    "\n",
    "def plot_feature_heatmap(df, numeric_features):\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numeric_features].corr()\n",
    "    \n",
    "    # Create heatmap using plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix,\n",
    "        x=numeric_features,\n",
    "        y=numeric_features,\n",
    "        colorscale='RdBu',\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        text=np.round(correlation_matrix, 2),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Feature Correlation Heatmap',\n",
    "        width=900,\n",
    "        height=900,\n",
    "        xaxis_tickangle=-45\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nüîç Strongest Feature Correlations:\")\n",
    "    correlations = []\n",
    "    for i in range(len(numeric_features)):\n",
    "        for j in range(i+1, len(numeric_features)):\n",
    "            corr = correlation_matrix.iloc[i,j]\n",
    "            if abs(corr) > 0.3: \n",
    "                correlations.append((\n",
    "                    numeric_features[i],\n",
    "                    numeric_features[j],\n",
    "                    corr\n",
    "                ))\n",
    "    \n",
    "    correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    for feat1, feat2, corr in correlations:\n",
    "        print(f\"{feat1} ‚ÜîÔ∏è {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Enhanced autoencoder architecture\n",
    "def build_autoencoder(input_dim, encoding_dim=8):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder with more layers and regularization\n",
    "    encoded = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(input_layer)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    \n",
    "    encoded = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    \n",
    "    # Bottleneck layer\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    \n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    \n",
    "    # Output layer\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Enhanced training process\n",
    "def train_autoencoder(autoencoder, X_scaled, epochs=50, batch_size=128):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        X_scaled, X_scaled,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,  # Increased validation split\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_autoencoder(autoencoder, X_scaled):\n",
    "    X_reconstructed = autoencoder.predict(X_scaled)\n",
    "    mse = mean_squared_error(X_scaled, X_reconstructed)\n",
    "    r2 = r2_score(X_scaled, X_reconstructed)\n",
    "    \n",
    "    print(f\"\\nüîπ Autoencoder Accuracy Metrics:\")\n",
    "    print(f\"   ‚úÖ Mean Squared Error (MSE): {mse:.5f}\")\n",
    "    print(f\"   ‚úÖ R¬≤ Score: {r2:.5f}\")\n",
    "\n",
    "def build_knn_model(latent_features, metric='cosine'):\n",
    "    knn = NearestNeighbors(metric=metric, algorithm='brute')\n",
    "    knn.fit(latent_features)\n",
    "    return knn\n",
    "\n",
    "def plot_tracks_by_genre(latent_features, df):\n",
    "    pca = PCA(n_components=3)\n",
    "    latent_3d = pca.fit_transform(latent_features)\n",
    "    \n",
    "    plot_df = pd.DataFrame(\n",
    "        latent_3d, \n",
    "        columns=['PC1', 'PC2', 'PC3']\n",
    "    )\n",
    "    plot_df['Genre'] = df['track_genre']\n",
    "    plot_df['Track'] = df['track_name']\n",
    "    plot_df['Artist'] = df['artists']\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        plot_df, \n",
    "        x='PC1', \n",
    "        y='PC2', \n",
    "        z='PC3',\n",
    "        color='Genre',\n",
    "        hover_data=['Track', 'Artist'],\n",
    "        title='3D Interactive Visualization of Tracks by Genre',\n",
    "        labels={'PC1': 'First Principal Component',\n",
    "                'PC2': 'Second Principal Component',\n",
    "                'PC3': 'Third Principal Component'}\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.85\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    genre_dist = df['track_genre'].value_counts()\n",
    "    print(\"\\nüéµ Genre Distribution:\")\n",
    "    for genre, count in genre_dist.items():\n",
    "        print(f\"{genre}: {count} tracks\")\n",
    "\n",
    "def get_track_recommendations(track_index, n_recommendations=5):\n",
    "    print(\"\\nüéµ Selected Track:\")\n",
    "    selected_track = df.iloc[track_index]\n",
    "    print(f\"Track: {selected_track['track_name']}\")\n",
    "    print(f\"Artist: {selected_track['artists']}\")\n",
    "    print(f\"Genre: {selected_track['track_genre']}\")\n",
    "\n",
    "    track_vector = latent_features[track_index].reshape(1, -1)\n",
    "    distances, indices = knn.kneighbors(track_vector, n_neighbors=20)  \n",
    "    similar_indices = indices.flatten()[1:]  \n",
    "    similar_tracks = df.iloc[similar_indices]\n",
    "\n",
    "    selected_genre = selected_track['track_genre']\n",
    "    filtered_tracks = similar_tracks[similar_tracks['track_genre'] != selected_genre]\n",
    "\n",
    "    recommendations = similar_tracks.head(n_recommendations)\n",
    "\n",
    "    print(\"\\nüé∂ Recommended Tracks (Different Genre):\")\n",
    "    return recommendations[['track_name', 'artists', 'track_genre']]\n",
    "\n",
    "def main():\n",
    "    global df, latent_features, knn\n",
    "    \n",
    "    df, X_scaled, numeric_features = load_and_preprocess_data()\n",
    "    plot_feature_heatmap(df, numeric_features)\n",
    "    input_dim = X_scaled.shape[1]  \n",
    "    encoding_dim = 8  # Increased encoding dimension\n",
    "    \n",
    "    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    print(\"üöÄ Training autoencoder with enhanced architecture...\")\n",
    "    history = train_autoencoder(autoencoder, X_scaled, epochs=50, batch_size=128)\n",
    "    \n",
    "    evaluate_autoencoder(autoencoder, X_scaled)\n",
    "\n",
    "    print(\"\\nüîç Generating latent representations...\")\n",
    "    latent_features = encoder.predict(X_scaled)\n",
    "    knn = build_knn_model(latent_features, metric='cosine')\n",
    "    \n",
    "    print(\"\\nüìä Generating genre distribution visualization...\")\n",
    "    plot_tracks_by_genre(latent_features, df)\n",
    "    \n",
    "    sample_track_index = 15\n",
    "    recommendations = get_track_recommendations(sample_track_index)\n",
    "    print(recommendations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Constants\n",
    "FEATURE_CORRELATION_THRESHOLD = 0.3\n",
    "IQR_OUTLIER_MULTIPLIER = 1.5\n",
    "AUTOENCODER_ENCODING_DIM = 8\n",
    "AUTOENCODER_EPOCHS = 50\n",
    "AUTOENCODER_BATCH_SIZE = 128\n",
    "AUTOENCODER_VALIDATION_SPLIT = 0.2\n",
    "KNN_METRIC = 'cosine'\n",
    "N_RECOMMENDATIONS = 5\n",
    "KNN_NEIGHBORS = 20  # Number of neighbors to consider for recommendations\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Loads the Spotify tracks dataset, preprocesses it, and returns\n",
    "    the processed DataFrame and scaled numeric features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame, NumPy array, list)\n",
    "               - DataFrame: The processed DataFrame.\n",
    "               - NumPy array: Scaled numeric features.\n",
    "               - list: List of numeric feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ds = load_dataset(\"maharshipandya/spotify-tracks-dataset\")\n",
    "        df = pd.DataFrame(ds[\"train\"])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading dataset: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Dataset loaded. First few rows:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df['track_genre_encoded'] = le.fit_transform(df['track_genre'])\n",
    "\n",
    "    numeric_features = [\n",
    "        'danceability', 'energy', 'loudness', 'speechiness',\n",
    "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
    "        'track_genre_encoded'\n",
    "    ]\n",
    "\n",
    "    df = df.dropna(subset=numeric_features).reset_index(drop=True)\n",
    "\n",
    "    for feature in numeric_features[:-1]:  # Skip the encoded genre\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        df = df[~((df[feature] < (Q1 - IQR_OUTLIER_MULTIPLIER * IQR)) |\n",
    "              (df[feature] > (Q3 + IQR_OUTLIER_MULTIPLIER * IQR)))]\n",
    "\n",
    "    X = df[numeric_features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return df.copy(), X_scaled.copy(), numeric_features  # Return copies to avoid unintended modifications\n",
    "\n",
    "\n",
    "def plot_feature_heatmap(df, numeric_features):\n",
    "    \"\"\"\n",
    "    Generates a heatmap visualizing the correlation between numeric features.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the data.\n",
    "        numeric_features (list): List of numeric feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    correlation_matrix = df[numeric_features].corr()\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix,\n",
    "        x=numeric_features,\n",
    "        y=numeric_features,\n",
    "        colorscale='RdBu',\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        text=np.round(correlation_matrix, 2),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Feature Correlation Heatmap',\n",
    "        width=900,\n",
    "        height=900,\n",
    "        xaxis_tickangle=-45\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    print(\"\\nüîç Strongest Feature Correlations:\")\n",
    "    correlations = []\n",
    "    for i in range(len(numeric_features)):\n",
    "        for j in range(i + 1, len(numeric_features)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > FEATURE_CORRELATION_THRESHOLD:\n",
    "                correlations.append((\n",
    "                    numeric_features[i],\n",
    "                    numeric_features[j],\n",
    "                    corr\n",
    "                ))\n",
    "\n",
    "    correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    for feat1, feat2, corr in correlations:\n",
    "        print(f\"{feat1} ‚ÜîÔ∏è {feat2}: {corr:.3f}\")\n",
    "\n",
    "\n",
    "def build_autoencoder(input_dim, encoding_dim=AUTOENCODER_ENCODING_DIM):\n",
    "    \"\"\"\n",
    "    Builds an autoencoder model for dimensionality reduction.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        encoding_dim (int): Dimensionality of the bottleneck layer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Model, Model) - The autoencoder and encoder models.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder with more layers and regularization\n",
    "    encoded = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(input_layer)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "    encoded = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "    # Bottleneck layer\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "    # Output layer\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, X_scaled, epochs=AUTOENCODER_EPOCHS, batch_size=AUTOENCODER_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Trains the autoencoder model.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (Model): The autoencoder model to train.\n",
    "        X_scaled (NumPy array): Scaled training data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        History: Training history object.\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        history = autoencoder.fit(\n",
    "            X_scaled, X_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=AUTOENCODER_VALIDATION_SPLIT,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error during autoencoder training: {e}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(autoencoder, X_scaled):\n",
    "    \"\"\"\n",
    "    Evaluates the autoencoder model's performance.\n",
    "\n",
    "    Args:\n",
    "        autoencoder (Model): The trained autoencoder model.\n",
    "        X_scaled (NumPy array): Scaled data for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    X_reconstructed = autoencoder.predict(X_scaled)\n",
    "    mse = mean_squared_error(X_scaled, X_reconstructed)\n",
    "    r2 = r2_score(X_scaled, X_reconstructed)\n",
    "\n",
    "    print(f\"\\nüîπ Autoencoder Accuracy Metrics:\")\n",
    "    print(f\"  ‚úÖ Mean Squared Error (MSE): {mse:.5f}\")\n",
    "    print(f\"  ‚úÖ R¬≤ Score: {r2:.5f}\")\n",
    "\n",
    "\n",
    "def build_knn_model(latent_features, metric=KNN_METRIC):\n",
    "    \"\"\"\n",
    "    Builds a KNN model for track recommendations.\n",
    "\n",
    "    Args:\n",
    "        latent_features (NumPy array): Latent features of the tracks.\n",
    "        metric (str): Distance metric for KNN.\n",
    "\n",
    "    Returns:\n",
    "        NearestNeighbors: Trained KNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    knn = NearestNeighbors(metric=metric, algorithm='brute')  # Consider experimenting with 'kd_tree' or 'ball_tree'\n",
    "    knn.fit(latent_features)\n",
    "    return knn\n",
    "\n",
    "\n",
    "def plot_tracks_by_genre(latent_features, df):\n",
    "    \"\"\"\n",
    "    Visualizes tracks in a 3D space using PCA, colored by genre.\n",
    "\n",
    "    Args:\n",
    "        latent_features (NumPy array): Latent features of the tracks.\n",
    "        df (DataFrame): DataFrame containing track information.\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    latent_3d = pca.fit_transform(latent_features)\n",
    "\n",
    "    plot_df = pd.DataFrame(\n",
    "        latent_3d,\n",
    "        columns=['PC1', 'PC2', 'PC3']\n",
    "    )\n",
    "    plot_df['Genre'] = df['track_genre']\n",
    "    plot_df['Track'] = df['track_name']\n",
    "    plot_df['Artist'] = df['artists']\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        plot_df,\n",
    "        x='PC1',\n",
    "        y='PC2',\n",
    "        z='PC3',\n",
    "        color='Genre',\n",
    "        hover_data=['Track', 'Artist'],\n",
    "        title='3D Interactive Visualization of Tracks by Genre',\n",
    "        labels={'PC1': 'First Principal Component',\n",
    "                'PC2': 'Second Principal Component',\n",
    "                'PC3': 'Third Principal Component'}\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.85\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    genre_dist = df['track_genre'].value_counts()\n",
    "    print(\"\\nüéµ Genre Distribution:\")\n",
    "    for genre, count in genre_dist.items():\n",
    "        print(f\"{genre}: {count} tracks\")\n",
    "\n",
    "\n",
    "def get_track_recommendations(track_index, n_recommendations=N_RECOMMENDATIONS):\n",
    "    \"\"\"\n",
    "    Recommends tracks similar to a given track.\n",
    "\n",
    "    Args:\n",
    "        track_index (int): Index of the track to find recommendations for.\n",
    "        n_recommendations (int): Number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing recommended tracks.\n",
    "    \"\"\"\n",
    "    if not 0 <= track_index < len(df):\n",
    "        raise ValueError(f\"track_index ({track_index}) is out of bounds (0, {len(df) - 1})\")\n",
    "\n",
    "    print(\"\\nüéµ Selected Track:\")\n",
    "    selected_track = df.iloc[track_index]\n",
    "    print(f\"Track: {selected_track['track_name']}\")\n",
    "    print(f\"Artist: {selected_track['artists']}\")\n",
    "    print(f\"Genre: {selected_track['track_genre']}\")\n",
    "\n",
    "    track_vector = latent_features[track_index].reshape(1, -1)\n",
    "    distances, indices = knn.kneighbors(track_vector, n_neighbors=KNN_NEIGHBORS)\n",
    "    similar_indices = indices.flatten()[1:]\n",
    "    similar_tracks = df.iloc[similar_indices]\n",
    "\n",
    "    selected_genre = selected_track['track_genre']\n",
    "    #print(f\"Selected Genre: {selected_genre}\") # Debugging\n",
    "    #print(f\"All Similar Tracks Genres: {similar_tracks['track_genre'].unique()}\") # Debugging\n",
    "    filtered_tracks = similar_tracks[similar_tracks['track_genre'] != selected_genre]\n",
    "\n",
    "    recommendations = filtered_tracks.head(n_recommendations)\n",
    "\n",
    "    print(\"\\nüé∂ Recommended Tracks (Different Genre):\")\n",
    "    return recommendations[['track_name', 'artists', 'track_genre']]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global df, latent_features, knn\n",
    "\n",
    "    try:\n",
    "        df, X_scaled, numeric_features = load_and_preprocess_data()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during data loading/preprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    plot_feature_heatmap(df, numeric_features)\n",
    "    input_dim = X_scaled.shape[1]\n",
    "\n",
    "    try:\n",
    "        autoencoder, encoder = build_autoencoder(input_dim)\n",
    "    except Exception as e:\n",
    "        print(f\"Error building autoencoder: {e}\")\n",
    "        return\n",
    "    print(\"üöÄ Training autoencoder with enhanced architecture...\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
